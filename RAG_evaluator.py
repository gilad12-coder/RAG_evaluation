from typing import Dict, Any, List
from enum import Enum
from distutils.util import strtobool
from chunk_based_metrics import ChunkAnalysis, split_into_chunks, calculate_utilization_score
from pydantic import BaseModel
from loguru import logger
import dspy


class EvaluationMetric(str, Enum):
    CONTEXT_ADHERENCE = "Context Adherence"
    CORRECTNESS = "Correctness"
    COMPLETENESS = "Completeness"
    INSTRUCTION_ADHERENCE = "Instruction Adherence"
    CONTEXT_RELEVANCE = "Context Relevance"


class MetricDescription(str, Enum):
    CONTEXT_ADHERENCE_DESC = (
        "Task: Evaluate if the answer strictly uses only the information provided in the context.\n"
        "Steps:\n"
        "1. Carefully read the context and the answer.\n"
        "2. Identify any information in the answer that is not present in the context.\n"
        "3. If the answer contains only information from the context, evaluate as 'passed'.\n"
        "4. If the answer includes any information not in the context, evaluate as 'failed'.\n"
    )
    CORRECTNESS_DESC = (
        "Task: Assess the factual accuracy of the answer based on the given context.\n"
        "Steps:\n"
        "1. Compare each statement in the answer to the information in the context.\n"
        "2. Check for any factual errors or contradictions between the answer and the context.\n"
        "3. If all statements in the answer are factually correct, evaluate as 'passed'.\n"
        "4. If any statement is incorrect or contradicts the context, evaluate as 'failed'.\n"
    )
    COMPLETENESS_DESC = (
        "Task: Determine if the answer fully addresses all aspects of the user's question using the context provided.\n"
        "Steps:\n"
        "1. Identify all parts of the user's question.\n"
        "2. Check if the answer addresses each part of the question.\n"
        "3. Verify that all relevant information from the context is included in the answer.\n"
        "4. If the answer covers all parts of the question and includes all relevant context, evaluate as 'passed'.\n"
        "5. If any part of the question is unanswered or relevant context is missing, evaluate as 'failed'.\n"
    )
    INSTRUCTION_ADHERENCE_DESC = (
        "Task: Evaluate if the answer follows the specific instructions or requirements in the user's question.\n"
        "Steps:\n"
        "1. Identify any specific instructions or requirements in the user's question.\n"
        "2. Check if the answer addresses these specific instructions or requirements.\n"
        "3. If the answer follows all instructions and meets all requirements, evaluate as 'passed'.\n"
        "4. If the answer fails to follow any instruction or meet any requirement, evaluate as 'failed'.\n"
    )
    CONTEXT_RELEVANCE_DESC = (
        "Task: Assess how relevant the provided context is to the user's question.\n"
        "Steps:\n"
        "1. Analyze the user's question and identify its key components.\n"
        "2. Examine the context and determine if it contains information related to these key components.\n"
        "3. If the context provides relevant information for answering the question, evaluate as 'passed'.\n"
        "4. If the context is unrelated or only tangentially related to the question, evaluate as 'failed'.\n"
    )


class MetricDetails(BaseModel):
    metric: EvaluationMetric
    description: MetricDescription


class RAGEvaluator(dspy.Signature):
    """Signature for the RAG evaluator."""
    user_question: str = dspy.InputField(desc="The question entered by the user.")
    context: List[str] = dspy.InputField(
        desc="The chunks of text retrieved by the search engine related to the user's question.")
    answer: str = dspy.InputField(
        desc="The response generated by the model based on the user's question and the provided context.")
    metric: MetricDetails = dspy.InputField(desc="The specific aspect of the response being evaluated.")
    evaluation_result: str = dspy.OutputField(
        desc="The outcome of the evaluation process, did the response meet the criteria for the selected metric? output a final result ('failed' or 'passed').")


class EvalParser(dspy.Signature):
    """Parse the evaluation result to return a boolean value. 'passed' maps to True, and 'failed' maps to False."""
    models_output = dspy.InputField(desc="The evaluation result containing the final result and explanation.")
    formatted_output = dspy.OutputField(
        desc="The final result of the model's output. return False if 'failed', True if 'passed'.")


class RAGEval(dspy.Module):
    """Module for RAG evaluation."""

    def __init__(self):
        super().__init__()
        self.generate_eval = dspy.TypedChainOfThought(RAGEvaluator)
        self.generate_chunk_level_eval = dspy.TypedChainOfThought(ChunkAnalysis)
        self.format_eval = dspy.ChainOfThought(EvalParser)

    def forward(self, user_question: str, context: List[str], answer: str, metric: MetricDetails) -> dspy.Prediction:
        """Perform the forward pass of the RAG evaluation."""
        logger.debug(f"Evaluating metric: {metric.metric}")

        # Generate overall evaluation
        eval_result = self.generate_eval(
            user_question=user_question,
            context=context,
            answer=answer,
            metric=metric
        ).evaluation_result
        logger.debug(f"Overall evaluation result: {eval_result}")

        # Format the evaluation result
        model = dspy.OpenAI(temperature=0.0)
        dspy.settings.configure(lm=model)
        formatted_eval_result = self.format_eval(models_output=str(eval_result)).formatted_output
        final_judgment = bool(strtobool(formatted_eval_result))
        logger.debug(f"Formatted evaluation result: {final_judgment}")

        return dspy.Prediction(final_judgment=final_judgment)


def evaluate_responses(user_question: str, context: str, answer: str, metrics: List[MetricDetails],
                       num_evaluations: int = 5) -> Dict[str, Any]:
    """Evaluate responses for all metrics with varying temperatures, considering majority-vote attribution."""

    logger.info(f"Starting evaluation for all metrics")
    results = {metric.metric.value: [] for metric in metrics}
    chunk_attribution_votes = {}

    for i in range(num_evaluations):
        temperature = i / (num_evaluations - 1)  # Adjust temperature for each iteration
        logger.debug(f"Evaluation {i + 1}/{num_evaluations} with temperature: {temperature}")

        # Reconfigure the OpenAI model with the new temperature
        model = dspy.OpenAI(temperature=temperature)
        dspy.settings.configure(lm=model)

        # Create a new RAGEval instance after reconfiguring the model
        evaluator = RAGEval()

        # Generate chunk-level evaluation once per temperature setting
        chunks = split_into_chunks(context, 128)
        logger.debug(f"Split context into {len(chunks)} chunks")

        for chunk in chunks:
            logger.debug(f"Evaluating chunk: '{chunk}...'")
            chunk_result = evaluator.generate_chunk_level_eval(
                user_question=user_question,
                context=[chunk],
                answer=answer
            ).analysis_result

            # Log the chunk attribution result with improved clarity
            impact_label = "high impact" if chunk_result else "low impact"
            logger.debug(
                f"Chunk labeled as {impact_label}.")

            # Track attribution votes for each chunk
            if chunk not in chunk_attribution_votes:
                chunk_attribution_votes[chunk] = 0
            if chunk_result:
                chunk_attribution_votes[chunk] += 1

        # Evaluate each metric using the same chunk-level evaluation
        for metric in metrics:
            result = evaluator(user_question, chunks, answer, metric).final_judgment
            results[metric.metric.value].append(result)
            logger.debug(f"Evaluation {i + 1} result for {metric.metric.value}: {result}")

    # Determine which chunks had high attribution by majority vote
    majority_attributed_chunks = [
        chunk for chunk, votes in chunk_attribution_votes.items()
        if votes > num_evaluations / 2  # Majority vote threshold
    ]
    logger.debug(f"{len(majority_attributed_chunks)} chunks determined to have high attribution by majority vote")

    # Calculate attribution scores for chunks with high attribution
    attributed_chunk_utilization_dict = {}
    for chunk in majority_attributed_chunks:
        chunk_utilization_score = calculate_utilization_score(chunk, answer)
        attributed_chunk_utilization_dict[chunk] = chunk_utilization_score
        logger.debug(f"Chunk: '{chunk}...' - Utilization Score: {chunk_utilization_score:.4f}")

    # Combine results and separate attribution scores
    combined_results = {
        "metric_results": {},
        "chunk_attribution_scores": attributed_chunk_utilization_dict
    }

    for metric, metric_results in results.items():
        if metric_results:
            average_score = sum(r for r in metric_results) / len(metric_results)
            combined_results["metric_results"][metric] = {
                "average_score": average_score,
                "individual_results": [r for r in metric_results]
            }
            logger.info(f"Evaluation complete for {metric}. Average score: {average_score:.2f}")
        else:
            combined_results["metric_results"][metric] = {"error": "Failed to evaluate this metric"}

    return combined_results


def evaluate_all_metrics(user_question: str, context: str, answer: str, num_evaluations: int = 5) -> Dict[str, Any]:
    """Evaluate all metrics for a given question, context, and answer with varying temperatures."""
    logger.info("Starting evaluation of all metrics")
    metrics = [
        MetricDetails(metric=EvaluationMetric.CONTEXT_ADHERENCE, description=MetricDescription.CONTEXT_ADHERENCE_DESC),
        MetricDetails(metric=EvaluationMetric.CORRECTNESS, description=MetricDescription.CORRECTNESS_DESC),
        MetricDetails(metric=EvaluationMetric.COMPLETENESS, description=MetricDescription.COMPLETENESS_DESC),
        MetricDetails(metric=EvaluationMetric.INSTRUCTION_ADHERENCE,
                      description=MetricDescription.INSTRUCTION_ADHERENCE_DESC),
        MetricDetails(metric=EvaluationMetric.CONTEXT_RELEVANCE, description=MetricDescription.CONTEXT_RELEVANCE_DESC),
    ]

    try:
        # Call the evaluate_responses function to get scores and attribution
        scores = evaluate_responses(user_question, context, answer, metrics, num_evaluations)

    except Exception as e:
        logger.error(f"Error evaluating metrics: {str(e)}")
        logger.exception("Full traceback:")
        scores = {"error": str(e)}

    return scores
